{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1o8ILMbBqvQWcw_AtH6pbA_OVFGMJ7I3W","authorship_tag":"ABX9TyN4TjUJrTXqFkvPWuw4f9u9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7I-kLZ6w6I7","executionInfo":{"status":"ok","timestamp":1652149204031,"user_tz":-480,"elapsed":27615,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}},"outputId":"101eabbd-7335-427b-8cdf-019c5639e859"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsmVJIOLxT6n","executionInfo":{"status":"ok","timestamp":1652149790012,"user_tz":-480,"elapsed":12377,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}},"outputId":"26963076-a859-4ffe-86c2-3f6e3e8edcde"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 6.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.2 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 44.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 26.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 56.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=4cb3e18f7472825d4950ce0d0244cf7540060bda4fad456830cf1ded610bdcc1\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"elNGyp-zw09s","executionInfo":{"status":"ok","timestamp":1652149843665,"user_tz":-480,"elapsed":342,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}},"outputId":"ef0983dc-f241-4759-96bd-acec4456f833"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/BERT\n"]}],"source":["cd /content/drive/MyDrive/BERT"]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')       # 隐藏警告！\n","from transformers import logging\n","logging.set_verbosity_warning()\n","import csv\n","import pandas as pd\n","import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import numpy as np\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","from tqdm import tqdm"],"metadata":{"id":"CH1OCodRxQ9i","executionInfo":{"status":"ok","timestamp":1652156037377,"user_tz":-480,"elapsed":7,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["torch.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KxJswQVNy56v","executionInfo":{"status":"ok","timestamp":1652149803517,"user_tz":-480,"elapsed":340,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}},"outputId":"a02bf18e-87ae-44cf-aa77-7942b08a0505"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.11.0+cu113'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYR6MC78zGuw","executionInfo":{"status":"ok","timestamp":1652149805041,"user_tz":-480,"elapsed":3,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}},"outputId":"1542ef92-cad7-4e6f-bcd0-53bd9e347826"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def read_tsv(input_file,columns):\n","    with open(input_file,\"r\",encoding=\"utf-8\") as file:\n","        lines = []\n","        count = 1\n","        for line in file:\n","            if len(line.strip().split(\"\\t\")) != 1:\n","                lines.append([count]+line.strip().split(\"\\t\"))\n","                count += 1\n","        df = pd.DataFrame(lines)\n","        df.columns = columns\n","    return df\n","\n","# 数据集读取\n","class bqDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    # 读取单个样本\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(int(self.labels[idx]))\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# 精度计算\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","# 训练函数\n","def train():\n","    model.train()\n","    total_train_loss = 0\n","    iter_num = 0\n","    total_iter = len(train_loader)\n","    for batch in train_loader:\n","        # 正向传播\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        total_train_loss += loss.item()\n","\n","        # 反向梯度信息\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 参数更新\n","        optim.step()\n","\n","        iter_num += 1\n","        if (iter_num % 100 == 0):\n","            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (\n","            epoch, iter_num, loss.item(), iter_num / total_iter * 100))\n","\n","    print(\"Epoch: %d, Average training loss: %.4f\" % (epoch, total_train_loss / len(train_loader)))\n","\n","\n","def validation():\n","    model.eval()\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    for batch in val_dataloader:\n","        with torch.no_grad():\n","            # 正常传播\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","\n","        loss = outputs[0]\n","        logits = outputs[1]\n","\n","        total_eval_loss += loss.item()\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = labels.to('cpu').numpy()\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n","    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n","    print(\"Average testing loss: %.4f\" % (total_eval_loss / len(val_dataloader)))\n","    print(\"-------------------------------\")\n","\n","def predict():\n","    model.eval()\n","    test_predict = []\n","    for batch in test_dataloader:\n","        with torch.no_grad():\n","            # 正常传播\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        \n","        loss = outputs[0]\n","        logits = outputs[1]\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = labels.to('cpu').numpy()\n","        test_predict += list(np.argmax(logits, axis=1).flatten())\n","        \n","    return test_predict"],"metadata":{"id":"hKTtfTIQxgwn","executionInfo":{"status":"ok","timestamp":1652156041064,"user_tz":-480,"elapsed":511,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["corpus_name = ['lcqmc', 'bq_corpus', 'paws-x-zh'][0]\n","\n","bq_train = read_tsv(\"data/\" + corpus_name + \"/train.tsv\", ['index', 'question1', 'question2', 'label'])  \n","# ---------------------------------------------\n","#  paws读取时 train set 少了，为 49129 （49401\n","# ---------------------------------------------\n","q1_train = bq_train['question1']\n","q2_train = bq_train['question2']\n","train_label = bq_train['label']\n","bq_val = read_tsv(\"data/\" + corpus_name + \"/dev.tsv\", ['index', 'question1', 'question2', 'label'])\n","q1_val = bq_val['question1']\n","q2_val = bq_val['question2']\n","val_label = bq_val['label']\n","bq_test = read_tsv(\"data/\" + corpus_name + \"/test.tsv\", ['index', 'question1', 'question2'])\n","bq_test['label'] = 0\n","q1_test = bq_test['question1']\n","q2_test = bq_test['question2']\n","test_label = bq_test['label']\n","\n","\n","# input_ids：字的编码\n","# token_type_ids：标识是第一个句子还是第二个句子\n","# attention_mask：标识是不是填充\n","\n","# transformers bert相关的模型使用和加载\n","from transformers import BertTokenizer\n","\n","model_name = 'bert-base-chinese'\n","\n","# 分词器，词典\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","train_encoding = tokenizer(list(q1_train), list(q2_train),\n","                            truncation=True, padding=True, max_length=100)\n","val_encoding = tokenizer(list(q1_val), list(q2_val),\n","                            truncation=True, padding=True, max_length=100)\n","test_encoding = tokenizer(list(q1_test), list(q2_test), \n","                            truncation=True, padding=True, max_length=100)\n","\n","train_dataset = bqDataset(train_encoding, list(train_label))\n","val_dataset = bqDataset(val_encoding, list(val_label))\n","test_dataset = bqDataset(test_encoding, list(test_label))\n","\n","from transformers import BertForNextSentencePrediction, AdamW, get_linear_schedule_with_warmup\n","\n","model = BertForNextSentencePrediction.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 单个读取到批量读取\n","train_loader = DataLoader(train_dataset, batch_size=16, num_workers=6, pin_memory=True, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, num_workers=6, pin_memory=True, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=16, num_workers=6, pin_memory=True, shuffle=True)\n","\n","# 优化方法\n","optim = AdamW(model.parameters(), lr=1e-5)\n","for epoch in range(1):\n","    print(\"------------Epoch: %d ----------------\" % epoch)\n","    train()\n","    validation()\n","    torch.save(model.state_dict(), f'model_{epoch}.pt')\n","\n","# 预测\n","test_label = predict()\n","with open('result/' + corpus_name + '.tsv', 'w') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['index', 'prediction'])\n","    for i in range(len(test_label)):\n","        tsv_writer.writerow([str(i), str(test_label[i])])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPG9EEPKxJTy","outputId":"d837a2e2-4e62-4e2d-e429-40f3078186e8"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["------------Epoch: 0 ----------------\n","epoth: 0, iter_num: 100, loss: 0.3819, 0.67%\n","epoth: 0, iter_num: 200, loss: 0.1078, 1.34%\n","epoth: 0, iter_num: 300, loss: 0.2957, 2.01%\n","epoth: 0, iter_num: 400, loss: 0.2816, 2.68%\n","epoth: 0, iter_num: 500, loss: 0.3192, 3.35%\n","epoth: 0, iter_num: 600, loss: 0.4785, 4.02%\n","epoth: 0, iter_num: 700, loss: 0.1929, 4.69%\n","epoth: 0, iter_num: 800, loss: 0.1609, 5.36%\n","epoth: 0, iter_num: 900, loss: 0.3505, 6.03%\n","epoth: 0, iter_num: 1000, loss: 0.0408, 6.70%\n","epoth: 0, iter_num: 1100, loss: 0.2830, 7.37%\n","epoth: 0, iter_num: 1200, loss: 0.4989, 8.04%\n","epoth: 0, iter_num: 1300, loss: 0.0529, 8.71%\n","epoth: 0, iter_num: 1400, loss: 0.0838, 9.38%\n","epoth: 0, iter_num: 1500, loss: 0.1795, 10.05%\n","epoth: 0, iter_num: 1600, loss: 0.1016, 10.72%\n","epoth: 0, iter_num: 1700, loss: 0.1392, 11.39%\n","epoth: 0, iter_num: 1800, loss: 0.3273, 12.06%\n","epoth: 0, iter_num: 1900, loss: 0.2368, 12.73%\n","epoth: 0, iter_num: 2000, loss: 0.2494, 13.40%\n","epoth: 0, iter_num: 2100, loss: 0.0600, 14.07%\n","epoth: 0, iter_num: 2200, loss: 0.2145, 14.74%\n","epoth: 0, iter_num: 2300, loss: 0.4268, 15.41%\n","epoth: 0, iter_num: 2400, loss: 0.2026, 16.08%\n","epoth: 0, iter_num: 2500, loss: 0.0701, 16.75%\n","epoth: 0, iter_num: 2600, loss: 0.5891, 17.42%\n","epoth: 0, iter_num: 2700, loss: 0.4168, 18.09%\n","epoth: 0, iter_num: 2800, loss: 0.3865, 18.76%\n","epoth: 0, iter_num: 2900, loss: 0.3443, 19.43%\n","epoth: 0, iter_num: 3000, loss: 0.0430, 20.10%\n","epoth: 0, iter_num: 3100, loss: 0.1547, 20.77%\n","epoth: 0, iter_num: 3200, loss: 0.4321, 21.44%\n","epoth: 0, iter_num: 3300, loss: 0.2073, 22.11%\n","epoth: 0, iter_num: 3400, loss: 0.1431, 22.78%\n","epoth: 0, iter_num: 3500, loss: 0.0348, 23.45%\n","epoth: 0, iter_num: 3600, loss: 0.1534, 24.12%\n","epoth: 0, iter_num: 3700, loss: 0.3813, 24.79%\n","epoth: 0, iter_num: 3800, loss: 0.1112, 25.46%\n","epoth: 0, iter_num: 3900, loss: 0.5831, 26.13%\n","epoth: 0, iter_num: 4000, loss: 0.1416, 26.80%\n","epoth: 0, iter_num: 4100, loss: 0.4459, 27.47%\n","epoth: 0, iter_num: 4200, loss: 0.5098, 28.14%\n","epoth: 0, iter_num: 4300, loss: 0.0259, 28.81%\n","epoth: 0, iter_num: 4400, loss: 0.1978, 29.48%\n","epoth: 0, iter_num: 4500, loss: 0.1521, 30.15%\n","epoth: 0, iter_num: 4600, loss: 0.3021, 30.82%\n","epoth: 0, iter_num: 4700, loss: 0.0680, 31.50%\n","epoth: 0, iter_num: 4800, loss: 0.0947, 32.17%\n","epoth: 0, iter_num: 4900, loss: 0.2743, 32.84%\n","epoth: 0, iter_num: 5000, loss: 0.2966, 33.51%\n","epoth: 0, iter_num: 5100, loss: 0.0169, 34.18%\n","epoth: 0, iter_num: 5200, loss: 0.3104, 34.85%\n","epoth: 0, iter_num: 5300, loss: 0.0785, 35.52%\n","epoth: 0, iter_num: 5400, loss: 0.2189, 36.19%\n","epoth: 0, iter_num: 5500, loss: 0.3488, 36.86%\n","epoth: 0, iter_num: 5600, loss: 0.1986, 37.53%\n","epoth: 0, iter_num: 5700, loss: 0.0485, 38.20%\n","epoth: 0, iter_num: 5800, loss: 0.1512, 38.87%\n","epoth: 0, iter_num: 5900, loss: 0.1452, 39.54%\n","epoth: 0, iter_num: 6000, loss: 0.2501, 40.21%\n","epoth: 0, iter_num: 6100, loss: 0.1053, 40.88%\n","epoth: 0, iter_num: 6200, loss: 0.0649, 41.55%\n","epoth: 0, iter_num: 6300, loss: 0.2026, 42.22%\n","epoth: 0, iter_num: 6400, loss: 0.1977, 42.89%\n","epoth: 0, iter_num: 6500, loss: 0.1229, 43.56%\n","epoth: 0, iter_num: 6600, loss: 0.0215, 44.23%\n","epoth: 0, iter_num: 6700, loss: 0.1838, 44.90%\n","epoth: 0, iter_num: 6800, loss: 0.4380, 45.57%\n","epoth: 0, iter_num: 6900, loss: 0.0327, 46.24%\n","epoth: 0, iter_num: 7000, loss: 0.2976, 46.91%\n","epoth: 0, iter_num: 7100, loss: 0.0586, 47.58%\n","epoth: 0, iter_num: 7200, loss: 0.1258, 48.25%\n","epoth: 0, iter_num: 7300, loss: 0.1023, 48.92%\n","epoth: 0, iter_num: 7400, loss: 0.2845, 49.59%\n","epoth: 0, iter_num: 7500, loss: 0.3533, 50.26%\n","epoth: 0, iter_num: 7600, loss: 0.0400, 50.93%\n","epoth: 0, iter_num: 7700, loss: 0.4097, 51.60%\n","epoth: 0, iter_num: 7800, loss: 0.1893, 52.27%\n","epoth: 0, iter_num: 7900, loss: 0.0816, 52.94%\n","epoth: 0, iter_num: 8000, loss: 0.3508, 53.61%\n","epoth: 0, iter_num: 8100, loss: 0.1769, 54.28%\n","epoth: 0, iter_num: 8200, loss: 0.3667, 54.95%\n","epoth: 0, iter_num: 8300, loss: 0.0777, 55.62%\n","epoth: 0, iter_num: 8400, loss: 0.1185, 56.29%\n","epoth: 0, iter_num: 8500, loss: 0.0324, 56.96%\n","epoth: 0, iter_num: 8600, loss: 0.0556, 57.63%\n","epoth: 0, iter_num: 8700, loss: 0.1961, 58.30%\n","epoth: 0, iter_num: 8800, loss: 0.0483, 58.97%\n","epoth: 0, iter_num: 8900, loss: 0.5166, 59.64%\n","epoth: 0, iter_num: 9000, loss: 0.2341, 60.31%\n","epoth: 0, iter_num: 9100, loss: 0.1518, 60.98%\n","epoth: 0, iter_num: 9200, loss: 0.9357, 61.65%\n","epoth: 0, iter_num: 9300, loss: 0.3932, 62.32%\n","epoth: 0, iter_num: 9400, loss: 0.1160, 62.99%\n","epoth: 0, iter_num: 9500, loss: 0.0164, 63.66%\n","epoth: 0, iter_num: 9600, loss: 0.0561, 64.33%\n","epoth: 0, iter_num: 9700, loss: 0.4365, 65.00%\n","epoth: 0, iter_num: 9800, loss: 0.2351, 65.67%\n","epoth: 0, iter_num: 9900, loss: 0.1974, 66.34%\n","epoth: 0, iter_num: 10000, loss: 0.1520, 67.01%\n","epoth: 0, iter_num: 10100, loss: 0.3491, 67.68%\n","epoth: 0, iter_num: 10200, loss: 0.3720, 68.35%\n","epoth: 0, iter_num: 10300, loss: 0.1647, 69.02%\n","epoth: 0, iter_num: 10400, loss: 0.2277, 69.69%\n","epoth: 0, iter_num: 10500, loss: 0.1741, 70.36%\n","epoth: 0, iter_num: 10600, loss: 0.1652, 71.03%\n","epoth: 0, iter_num: 10700, loss: 0.0219, 71.70%\n","epoth: 0, iter_num: 10800, loss: 0.1772, 72.37%\n","epoth: 0, iter_num: 10900, loss: 0.2339, 73.04%\n","epoth: 0, iter_num: 11000, loss: 0.1338, 73.71%\n","epoth: 0, iter_num: 11100, loss: 0.2237, 74.38%\n","epoth: 0, iter_num: 11200, loss: 0.1224, 75.05%\n","epoth: 0, iter_num: 11300, loss: 0.0509, 75.72%\n","epoth: 0, iter_num: 11400, loss: 0.1297, 76.39%\n","epoth: 0, iter_num: 11500, loss: 0.2202, 77.06%\n","epoth: 0, iter_num: 11600, loss: 0.1639, 77.73%\n","epoth: 0, iter_num: 11700, loss: 0.6328, 78.40%\n","epoth: 0, iter_num: 11800, loss: 0.0827, 79.07%\n","epoth: 0, iter_num: 11900, loss: 0.0859, 79.74%\n","epoth: 0, iter_num: 12000, loss: 0.2844, 80.41%\n","epoth: 0, iter_num: 12100, loss: 0.2014, 81.08%\n","epoth: 0, iter_num: 12200, loss: 0.0393, 81.75%\n","epoth: 0, iter_num: 12300, loss: 0.0229, 82.42%\n","epoth: 0, iter_num: 12400, loss: 0.0169, 83.09%\n","epoth: 0, iter_num: 12500, loss: 0.1750, 83.76%\n","epoth: 0, iter_num: 12600, loss: 0.2161, 84.43%\n","epoth: 0, iter_num: 12700, loss: 0.3565, 85.10%\n","epoth: 0, iter_num: 12800, loss: 0.3216, 85.77%\n","epoth: 0, iter_num: 12900, loss: 0.2822, 86.44%\n","epoth: 0, iter_num: 13000, loss: 0.1682, 87.11%\n","epoth: 0, iter_num: 13100, loss: 0.0849, 87.78%\n","epoth: 0, iter_num: 13200, loss: 0.3210, 88.45%\n","epoth: 0, iter_num: 13300, loss: 0.2181, 89.12%\n","epoth: 0, iter_num: 13400, loss: 0.0643, 89.79%\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"H2zGvJCWiaNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dDtQWDSBiaK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rSX-VCF6iaGI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HhOhd0CSiZ_C","executionInfo":{"status":"ok","timestamp":1652162148182,"user_tz":-480,"elapsed":421,"user":{"displayName":"Fakes Fynn","userId":"06339103960293917197"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mE8GXSX0iZ8i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zaXWDTYqiZ5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"3P5JtPviiZ1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SLgQp-axiZzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uGNu45fWiZt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"kxql0oTGiZrC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7oUB0QrdiZg1"},"execution_count":null,"outputs":[]}]}